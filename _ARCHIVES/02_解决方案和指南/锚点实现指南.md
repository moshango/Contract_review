# 🔧 锚点定位问题修复实施指南

**文档版本**: v1.0
**修复等级**: CRITICAL
**实施周期**: 1.5 小时（快速修复）+ 后续优化

---

## 🎯 快速修复流程（1.5 小时）

### 步骤 1：修复锚点生成确定性（1 小时）

**问题**: 当前使用 `System.currentTimeMillis()` 导致同一文档每次生成不同的锚点 ID。

**文件**: `DocxUtils.java`

#### 1.1 查找并替换生成锚点的方法

首先定位当前的 `generateAnchorId()` 方法。替换为以下内容：

```java
/**
 * 生成确定性锚点ID
 *
 * ✨ 修复说明:
 * - 原方法使用时间戳，导致同一文档不同时间生成不同ID
 * - 新方法使用条款内容哈希，确保幂等性
 * - 同一文档在任何时间解析都生成相同的锚点ID
 *
 * @param clause 条款对象
 * @return 稳定的锚点ID，格式: anc-c1-8f3a1b2c
 */
public String generateAnchorId(Clause clause) {
    try {
        // 使用条款ID + 标题 + 前100字符内容作为种子
        String heading = clause.getHeading() != null ? clause.getHeading() : "";
        String text = clause.getText();
        int contentLen = Math.min(100, text != null ? text.length() : 0);
        String content = text != null ? text.substring(0, contentLen) : "";

        // 构造确定性输入
        String hashInput = clause.getId() + "|" + heading + "|" + content;

        // 使用 SHA-256 产生稳定的哈希
        MessageDigest md = MessageDigest.getInstance("SHA-256");
        byte[] hashBytes = md.digest(hashInput.getBytes(StandardCharsets.UTF_8));

        // 取前8位（4个字节）的十六进制表示
        StringBuilder hashHex = new StringBuilder();
        for (int i = 0; i < 4; i++) {
            hashHex.append(String.format("%02x", hashBytes[i] & 0xFF));
        }

        String anchorId = "anc-" + clause.getId() + "-" + hashHex.toString();
        logger.debug("生成确定性锚点: clauseId={}, anchorId={}, heading={}",
                     clause.getId(), anchorId, heading);

        return anchorId;
    } catch (NoSuchAlgorithmException e) {
        logger.error("SHA-256 算法不可用，回退到随机锚点", e);
        // 回退方案：使用随机数（仅在特殊环境中）
        return "anc-" + clause.getId() + "-" + UUID.randomUUID().toString().substring(0, 8);
    }
}
```

#### 1.2 更新调用点

在 `DocxUtils.java` 中查找所有调用 `generateAnchorId()` 的位置。

**原代码**（假设）:
```java
clause.setAnchorId(generateAnchorId(clause.getId()));  // ❌ 旧方式
```

**修改为**:
```java
clause.setAnchorId(generateAnchorId(clause));  // ✓ 新方式，传入完整 Clause 对象
```

需要修改的位置通常在：
- `extractClausesWithCorrectIndex()` 方法中
- `parseContractWithDocument()` 方法中
- 任何其他调用 `generateAnchorId()` 的地方

#### 1.3 添加所需的 import

```java
import java.security.MessageDigest;
import java.security.NoSuchAlgorithmException;
import java.nio.charset.StandardCharsets;
import java.util.UUID;
```

#### 1.4 验证修改

```bash
# 编译测试
mvn clean compile

# 运行单元测试（如果有）
mvn test
```

**预期结果**：编译成功，无错误。

---

### 步骤 2：扩展缓存 TTL（30 分钟）

**问题**: 30 分钟的缓存 TTL 不足以覆盖完整的 Parse → ChatGPT 审查 → Annotate 流程。

**文件**: `ParseResultCache.java`

#### 2.1 修改 TTL 配置

定位第 94 行，修改为：

**原代码**:
```java
private static final long DEFAULT_TTL_MINUTES = 30;
```

**修改为**:
```java
private static final long DEFAULT_TTL_MINUTES = 240;  // 从 30 分钟改为 4 小时
```

#### 2.2 添加日志记录（可选增强）

在 `retrieve()` 方法中的日志输出中添加 TTL 信息：

```java
logger.info("【缓存】Parse 结果已检索: parseResultId={}, 年龄={} 秒, " +
           "条款数={}, 文件名={}, TTL限制={} 分钟",
           cacheId, result.getAgeSeconds(), result.parseResult.getClauses().size(),
           result.sourceFilename, DEFAULT_TTL_MINUTES);
```

#### 2.3 验证修改

```bash
mvn clean compile
```

**预期结果**：编译成功。

---

## 🧪 测试验证

完成上述两个修复后，执行以下测试：

### 测试 1：锚点确定性验证

```bash
# 终端 1: 启动服务
cd D:\工作\合同审查系统开发\spring boot\Contract_review
mvn spring-boot:run

# 终端 2: 执行测试脚本
# 创建文件: test_anchor_consistency.sh

#!/bin/bash
CONTRACT_FILE="path/to/test_contract.docx"
ENDPOINT="http://localhost:8080/chatgpt/generate-prompt"

echo "=== 测试 1: 锚点确定性验证 ==="

# 第一次调用
echo "第一次 Parse..."
RESPONSE1=$(curl -s -X POST "$ENDPOINT" \
  -F "file=@$CONTRACT_FILE" \
  -F "contractType=通用合同" \
  -F "anchors=generate")

ANCHORS1=$(echo "$RESPONSE1" | grep -o '"anchorId":"[^"]*"' | head -5)
echo "第一次生成的锚点:"
echo "$ANCHORS1"

# 等待 5 秒
sleep 5

# 第二次调用
echo "第二次 Parse（应生成相同的锚点）..."
RESPONSE2=$(curl -s -X POST "$ENDPOINT" \
  -F "file=@$CONTRACT_FILE" \
  -F "contractType=通用合同" \
  -F "anchors=generate")

ANCHORS2=$(echo "$RESPONSE2" | grep -o '"anchorId":"[^"]*"' | head -5)
echo "第二次生成的锚点:"
echo "$ANCHORS2"

# 比较
if [ "$ANCHORS1" == "$ANCHORS2" ]; then
  echo "✓ 锚点一致性验证成功！"
  exit 0
else
  echo "✗ 锚点不一致！"
  echo "第一次: $ANCHORS1"
  echo "第二次: $ANCHORS2"
  exit 1
fi
```

执行：
```bash
bash test_anchor_consistency.sh
```

**预期输出**:
```
✓ 锚点一致性验证成功！
```

### 测试 2：缓存 TTL 验证

```bash
# 创建文件: test_cache_ttl.sh

#!/bin/bash
CONTRACT_FILE="path/to/test_contract.docx"
GENERATE_ENDPOINT="http://localhost:8080/chatgpt/generate-prompt"
IMPORT_ENDPOINT="http://localhost:8080/chatgpt/import-result-xml"

echo "=== 测试 2: 缓存 TTL 验证 ==="

# Step 1: Parse 并获取 parseResultId
echo "Step 1: 执行 Parse 获取 parseResultId..."
PARSE_RESPONSE=$(curl -s -X POST "$GENERATE_ENDPOINT" \
  -F "file=@$CONTRACT_FILE" \
  -F "contractType=通用合同" \
  -F "anchors=generate")

PARSE_RESULT_ID=$(echo "$PARSE_RESPONSE" | grep -o '"parseResultId":"[^"]*"' | cut -d'"' -f4)
echo "parseResultId: $PARSE_RESULT_ID"

if [ -z "$PARSE_RESULT_ID" ]; then
  echo "✗ 无法获取 parseResultId"
  exit 1
fi

# Step 2: 创建模拟的 ChatGPT 审查结果
REVIEW_JSON='{
  "issues": [
    {
      "clauseId": "c1",
      "severity": "HIGH",
      "category": "测试",
      "finding": "测试问题",
      "suggestion": "测试建议",
      "targetText": "第一条",
      "matchPattern": "EXACT"
    }
  ]
}'

# Step 3: 等待 50 分钟后调用 Annotate（超过新的 4 小时 TTL 前）
echo "Step 2: 等待 5 秒后执行 Annotate（实际环境等待 50 分钟）..."
sleep 5

echo "Step 3: 使用 parseResultId 执行 Annotate..."
ANNOTATE_RESPONSE=$(curl -s -X POST "$IMPORT_ENDPOINT?parseResultId=$PARSE_RESULT_ID&anchorStrategy=preferAnchor&cleanupAnchors=true" \
  -F "chatgptResponse=$REVIEW_JSON")

# 检查响应是否包含"使用缓存"日志或直接返回注释的文档
if echo "$ANNOTATE_RESPONSE" | grep -q "缓存的带锚点文档" || \
   file - <<< "$ANNOTATE_RESPONSE" | grep -q "Microsoft Word"; then
  echo "✓ 缓存 TTL 验证成功！缓存在规定时间内有效"
  exit 0
else
  echo "⚠️ 缓存可能已过期或使用了文件回退"
  echo "响应头: $(echo "$ANNOTATE_RESPONSE" | head -c 200)"
  exit 1
fi
```

执行：
```bash
bash test_cache_ttl.sh
```

**预期输出**:
```
✓ 缓存 TTL 验证成功！缓存在规定时间内有效
```

### 测试 3：完整工作流验证

```bash
#!/bin/bash
# 完整的 ChatGPT 工作流验证

CONTRACT_FILE="path/to/test_contract.docx"
GENERATE_ENDPOINT="http://localhost:8080/chatgpt/generate-prompt"
IMPORT_ENDPOINT="http://localhost:8080/chatgpt/import-result-xml"
OUTPUT_FILE="annotated_output.docx"

echo "=== 完整工作流测试 ==="

# 1. Parse
echo "1️⃣ Parse 阶段..."
PARSE_RESPONSE=$(curl -s -X POST "$GENERATE_ENDPOINT" \
  -F "file=@$CONTRACT_FILE" \
  -F "contractType=通用合同" \
  -F "anchors=generate")

PARSE_RESULT_ID=$(echo "$PARSE_RESPONSE" | grep -o '"parseResultId":"[^"]*"' | cut -d'"' -f4)
ANCHORS=$(echo "$PARSE_RESPONSE" | grep -o '"anchorId":"[^"]*"' | wc -l)

echo "  ✓ 获得 parseResultId: $PARSE_RESULT_ID"
echo "  ✓ 提取到 $ANCHORS 个锚点"

# 2. ChatGPT 审查（模拟）
echo "2️⃣ ChatGPT 审查阶段（模拟）..."
REVIEW_JSON='{
  "issues": [
    {
      "clauseId": "c1",
      "severity": "HIGH",
      "category": "测试类别",
      "finding": "这是一个测试问题",
      "suggestion": "这是一个测试建议",
      "targetText": "条款内容中的任何文字",
      "matchPattern": "EXACT"
    }
  ]
}'
echo "  ✓ 准备好审查 JSON"

# 3. Annotate
echo "3️⃣ Annotate 阶段..."
curl -s -X POST "$IMPORT_ENDPOINT?parseResultId=$PARSE_RESULT_ID&anchorStrategy=preferAnchor&cleanupAnchors=true" \
  -F "chatgptResponse=$REVIEW_JSON" \
  -o "$OUTPUT_FILE"

if [ -f "$OUTPUT_FILE" ] && [ -s "$OUTPUT_FILE" ]; then
  FILE_SIZE=$(stat -f%z "$OUTPUT_FILE" 2>/dev/null || stat -c%s "$OUTPUT_FILE")
  echo "  ✓ 生成带批注的文档: $OUTPUT_FILE ($FILE_SIZE 字节)"
  echo ""
  echo "✅ 完整工作流验证成功！"
  exit 0
else
  echo "  ✗ 生成文档失败"
  exit 1
fi
```

---

## 📋 提交修改清单

修改完成后，按以下检查清单验证：

- [ ] **DocxUtils.java** 已修改锚点生成方法
  - [ ] 导入了 `java.security.MessageDigest`
  - [ ] 使用 SHA-256 替代时间戳
  - [ ] 更新了所有调用点
  - [ ] 编译无错误

- [ ] **ParseResultCache.java** 已更新 TTL
  - [ ] `DEFAULT_TTL_MINUTES` 改为 240
  - [ ] 编译无错误

- [ ] **测试验证**
  - [ ] 本地编译成功
  - [ ] 锚点确定性测试通过
  - [ ] 缓存 TTL 测试通过
  - [ ] 完整工作流测试通过

- [ ] **代码审查**
  - [ ] 无废弃代码
  - [ ] 日志记录充分
  - [ ] 异常处理完善

---

## 🚀 部署步骤

### 本地验证完毕后：

```bash
# 1. 清理构建
mvn clean

# 2. 完整编译
mvn compile

# 3. 运行测试
mvn test

# 4. 构建打包
mvn package -DskipTests

# 5. 部署到生产环境
# （根据你的部署流程）
```

---

## 📝 后续优化（可选）

修复 1 + 2 完成后，可继续进行以下优化（非紧急）：

### 优先级 3：锚点验证（2 小时）

在 `WordXmlCommentProcessor.java` 中添加锚点验证，确保 review JSON 中的锚点存在。

### 优先级 4：统一解析器（3 小时）

统一所有地方使用 `extractClausesWithCorrectIndex()`，移除冗余的解析方法。

---

## 🆘 故障排除

### 问题：编译失败 "MessageDigest cannot be resolved"

**解决**: 确保在文件顶部添加了所需的 import：

```java
import java.security.MessageDigest;
import java.security.NoSuchAlgorithmException;
import java.nio.charset.StandardCharsets;
```

### 问题：锚点仍然不一致

**排查**:
1. 检查是否应用了所有修改
2. 清理缓存，重新启动服务：
   ```bash
   mvn clean
   mvn spring-boot:run
   ```
3. 查看日志确认新方法是否被调用

### 问题：缓存检索仍然显示过期

**排查**:
1. 确认 `DEFAULT_TTL_MINUTES` 已改为 240
2. 重新编译：`mvn clean compile`
3. 检查日志中的时间戳

---

## 📊 预期效果

完成这两个关键修复后：

| 指标 | 修复前 | 修复后 |
|------|--------|--------|
| 锚点确定性 | ❌ 不稳定 | ✅ 稳定 |
| 工作流失败率 | 30-40% | < 5% |
| 缓存有效期 | 30 分钟 | 4 小时 |
| 用户体验 | 经常失败 | 基本稳定 |

---

## ✅ 完成标志

当以下条件满足时，修复完成：

1. ✅ 同一文档在不同时间生成相同的锚点 ID
2. ✅ 缓存能存活 4 小时
3. ✅ 日志显示"使用缓存的带锚点文档"而非"使用用户上传的文件"
4. ✅ 完整的 ChatGPT 工作流（Parse → Annotate）成功率 > 95%

